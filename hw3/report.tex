\documentclass{article}

\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}

\pagestyle{fancy}

\author{
  Robin Touche \\
  \and
  Fredrik Bredmar
}

\title{Machine Learning Homework 3}

\begin{document}

\maketitle

\setcounter{section}{1}
\subsection{}
\paragraph{a)}

Since we have a normal distribution the random variable can take on any real
value. Thus the state space is $\mathbb{R}^2$.

\paragraph{b)}

% $(X_1, X_2 = x_2) = \mathcal{N} \left( \mu_1 + \frac{\sigma_1}{\sigma_2}\rho \left( x_2 - \mu_2 \right), \left( 1 - \rho^2 \right) \sigma_1^2 \right)$

\paragraph{c)}

% TODO
%
% For each iteration we calculate the conditional probability of $(X_1 \vert X_2
% = x_2)$, which becomes a normal distribution. Then we take a random sample from
% this distribution which we will call $x_1$.
%
% The next iteration we instead calculate and sample from $(X_2 \vert X_1 = x_1)$
% with the $x_1$ we just aquired and set this sample as $x_2$ for the next
% iteration where we repeat the prevoius step.

\setcounter{section}{2}
\subsection*{2.1}

See code.

\subsection*{2.2}

\paragraph{a}

Equation (1) in the assignment description is the conditional posterior
distribution that looks as follows.

\begin{equation*}
P(Z_i = k \vert Z_{\neg i}, W) \propto P(w_i \vert z_i = k, z_{\neg i}, w_{\neg i})P(z_i = k \vert z_{\neg i })
\end{equation*}

So the first term on the right hand side is the likelihood function and the
second term is the prior. Then by integrating the first term over $\beta$ we
end up with;

\begin{equation*}
P(w_i \vert z_i = j, z_{\neg i}, w_{\neg i}) = \frac{ N^{w_i}_{k,\neg i} + \eta }{V\eta + \sum_{w^{\theta}=1}^V N^{w^\theta}_{k, \neg i}}
\end{equation*}

which is the first part of equation (1) in the assignment description. In
similar way but when integrating over $\theta$ we get the following expression
for the second term in equation (2) (also from the assignment).

\begin{equation*}
P(z_i = k \vert z_{\neg i}) = \frac{\alpha + M^{k}_{d,\neg i}}{T\alpha + \sum_{k^\theta=1}^T M^k_{d, \neg i} }
\end{equation*}

As we see we end up with equation (2) if we replace the right hand side with
the corresponding equations stated above.

\paragraph{b}

$\beta$ represents what words related to each topic. For a given topic k,
$\beta_k$ is a distribution of how likely each word is to appear with the
topic. Likewise, for a given word, $\beta$ represents what topics this word was
assigned to and how often.

In the same vein $\theta$ represents the distribution of topics in each
document and vice versa.

\paragraph{c}
\paragraph{d}
\paragraph{e}

We use a Gibbs sampler to assign topics to words. This means that since we
sample there's going to be a random element to each run. As such we can only
work with the expected value of the distributions.

\paragraph{f}

This does not matter that much because as the number of iterations increases we
will approach the expected value. If the run the algorithm enough times the
approximation will be very close to the exact values.

\end{document}
