\documentclass{article}

\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}

\pagestyle{fancy}

\author{
  Robin Touche \\
  \and
  Fredrik Bredmar
}

\title{Machine Learning Homework 3}

\begin{document}

\maketitle

\subsection*{1.1}
\paragraph{a)}

Since we have a normal distribution the random variable can take on any real
value. Thus the state space is $\mathbb{R}^2$.

\paragraph{b)}

% $(X_1, X_2 = x_2) = \mathcal{N} \left( \mu_1 + \frac{\sigma_1}{\sigma_2}\rho \left( x_2 - \mu_2 \right), \left( 1 - \rho^2 \right) \sigma_1^2 \right)$

\paragraph{c)}

The marginal distributions are Gaussian as well. To sample from a normal
distribution ($\emph{i.e.}$ with $\mu = 0$ and $\sigma^2 = 1$) we can for
instance use the Box-Muller method. Which states that, given two values, u and
v, uniformly distributed between $0$ and $1$, we can obtain a normally
distributed value x as:

\begin{equation}
  x = \sqrt{-2 \ln u } \cdot \cos \left(2 \pi v \right)
\end{equation}

Alternatively, $\sin$ instead of $\cos$ gives another value. \newline
Now that we have this normally distributed value we can scale this to the
gaussian distribution as $\sigma x + \mu$.

\subsection*{2.1}

See code.

\subsection*{2.2}

\paragraph{a}

Equation (1) in the assignment description is the conditional posterior
distribution that looks as follows.

\begin{equation*}
P(Z_i = k \vert Z_{\neg i}, W) \propto P(w_i \vert z_i = k, z_{\neg i}, w_{\neg i})P(z_i = k \vert z_{\neg i })
\end{equation*}

So the first term on the right hand side is the likelihood function and the
second term is the prior. Then by integrating the first term over $\beta$ we
end up with;

\begin{equation*}
P(w_i \vert z_i = j, z_{\neg i}, w_{\neg i}) = \frac{ N^{w_i}_{k,\neg i} + \eta }{V\eta + \sum_{w^{\theta}=1}^V N^{w^\theta}_{k, \neg i}}
\end{equation*}

which is the first part of equation (1) in the assignment description. In
similar way but when integrating over $\theta$ we get the following expression
for the second term in equation (2) (also from the assignment).

\begin{equation*}
P(z_i = k \vert z_{\neg i}) = \frac{\alpha + M^{k}_{d,\neg i}}{T\alpha + \sum_{k^\theta=1}^T M^k_{d, \neg i} }
\end{equation*}

As we see we end up with equation (2) if we replace the right hand side with
the corresponding equations stated above.

\paragraph{b}

$\beta$ represents what words related to each topic. For a given topic k,
$\beta_k$ is a distribution of how likely each word is to appear with the
topic. Likewise, for a given word, $\beta$ represents what topics this word was
assigned to and how often.

In the same vein $\theta$ represents the distribution of topics in each
document and vice versa.

\paragraph{c}
\paragraph{d}
\paragraph{e}

We use a Gibbs sampler to assign topics to words. This means that since we
sample there's going to be a random element to each run. As such we can only
work with the expected value of the distributions.

\paragraph{f}

This does not matter that much because as the number of iterations increases we
will approach the expected value. If the run the algorithm enough times the
approximation will be very close to the exact values.

\end{document}
