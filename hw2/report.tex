\documentclass{article}

\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{amsmath}

\pagestyle{fancy}

\author{
  Robin Touche \\
  \and
  Fredrik Bredmar
}

\title{Machine Learning Homework 2}

% \lhead{Robin Touche 900610-3270}
% \rhead{robint@student.chalmers.se}

\begin{document}

\maketitle

\chapter{Homework 2}
\setcounter{section}{1}
\subsection{}
\paragraph{a)}

Using Bayes' theorem we can calculate the probability $P\big(C = 1 \vert (0, 1, 1)\big)$ as

\begin{align}
  P\left( t_{new} = 1 \vert \mathbf{X}, \mathbf{t}, \mathbf{x}_{new} \right)
  = \frac{P \left( \mathbf{x}_{new} \vert t_{new} = 1, \mathbf{X}, \mathbf{t} \right) P \left( t_{new} = 1 \right)}{ \sum_j P \left( \mathbf{x}_{new} \vert t_{new} = j, \mathbf{X}, \mathbf{t} \right) P \left( t_{new} = j \right) }
\end{align}

We chose the prior $P ( t_{new} = 1 )$ as $\frac{1}{2}$ since we have any
domain knowledge.

Since we have binary features we can use the Bernoulli product model to
calculate the likelihood as

\begin{align}
  P \left( \mathbf{x}_{new} \vert t_{new} = 1, \mathbf{X}, \mathbf{t} \right) = \prod_{i = 1}^{3} P_i^{x_i} \left( 1 - P_i \right)^{1 - x_i}
\end{align}

where $P_i$ is the probability of feature $x_i$ is in class $t_{new} = 1$
($\emph{i.e}$ that the person with that feature is content).  For the person
given in the assignment this calcualtes to $\frac{1}{4} \cdot \frac{1}{2} \cdot
\frac{3}{4} = \frac{3}{32}$

Since we chose a prior as a uniform $\frac{1}{2}$ the terms in the numerator
and denominator cancels out and the denomimator simply becomes the total
probability of vector $\mathbf{x}$ appearing in any class which calculates to

\begin{align}
  \frac{1}{4} \cdot \frac{1}{2} \cdot \frac{3}{4} + \frac{3}{4} \cdot \frac{1}{4} \cdot \frac{1}{4}  = \frac{3}{32} + \frac{3}{64} = \frac{9}{64}
\end{align}

So the final probability that this person is content thus becomes

\begin{align}
  \frac{\frac{3}{32}}{\frac{9}{64}} = \frac{2}{3}
\end{align}

\paragraph{b)}

We can calculate this the same way as in $a)$. The only difference is that the
probability for $\mathbf{x}_{new}$ being content (or not content) now becomes
the probability of that person being healthy and content plus the probability
of that person not being healty but content.

In this case this is equivalent to us simply ignoring the ''healthy'' variable
and just doing te calculation on the remaining two.

The final probablity becomes

\begin{align}
  \frac{\frac{1}{8}}{\frac{1}{8} + \frac{3}{16}} = \frac{2}{5}
\end{align}

\subsection{}

\setcounter{section}{2}
\setcounter{subsection}{0}
\subsection{}

\subsection{}
\end{document}
