\documentclass{article}

\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{amsmath}

\pagestyle{fancy}

\author{
  Robin Touche \\
  \and
  Fredrik Bredmar
}

\title{Machine Learning Homework 2}

\begin{document}

\maketitle

\setcounter{section}{1}
\subsection{}
\paragraph{a)}

Using Bayes' theorem we can calculate the probability $P\big(C = 1 \vert (0, 1, 1)\big)$ as

\begin{align}
  P\left( t_{new} = 1 \vert \mathbf{X}, \mathbf{t}, \mathbf{x}_{new} \right)
  = \frac{P \left( \mathbf{x}_{new} \vert t_{new} = 1, \mathbf{X}, \mathbf{t} \right) P \left( t_{new} = 1 \right)}{ \sum_j P \left( \mathbf{x}_{new} \vert t_{new} = j, \mathbf{X}, \mathbf{t} \right) P \left( t_{new} = j \right) }
\end{align}

We chose the prior $P ( t_{new} = 1 )$ as $\frac{1}{2}$ since we do not have any
domain knowledge.

The features are binary so we can use the Bernoulli product model to
calculate the likelihood as

\begin{align}
  P \left( \mathbf{x}_{new} \vert t_{new} = 1, \mathbf{X}, \mathbf{t} \right) = \prod_{i = 1}^{3} P_i^{x_i} \left( 1 - P_i \right)^{1 - x_i}
\end{align}

where $P_i$ is the probability of feature $x_i$ is in class $t_{new} = 1$
($\emph{i.e}$ that the person with that feature is content).  For the person
given in the assignment this calcualtes to $\frac{1}{4} \cdot \frac{1}{2} \cdot
\frac{3}{4} = \frac{3}{32}$

Since the prior is uniform with probability $\frac{1}{2}$ the terms in the numerator
and denominator cancel out and the denomimator simply become the total
probability of vector $\mathbf{x}$ appearing in any class which calculates to

\begin{align}
  \frac{1}{4} \cdot \frac{1}{2} \cdot \frac{3}{4} + \frac{3}{4} \cdot \frac{1}{4} \cdot \frac{1}{4}  = \frac{3}{32} + \frac{3}{64} = \frac{9}{64}
\end{align}

The final probability that this person is content thus become

\begin{align}
  \frac{\frac{3}{32}}{\frac{9}{64}} = \frac{2}{3}
\end{align}

\paragraph{b)}

To calculate this we need to add the posterior of each case (\emph{i.e.} when
$x_3 = 0$ and when $x_3 = 1$) times the probability of that case ocurring in
the first place.

\begin{align}
  P\left( t_{new} = 1 \vert \mathbf{X}, \mathbf{t}, \mathbf{x}_{new} = (0, 1, 0) \right) P \left( x_3 = 0  \vert x_1, x_2 \right) \cdot \\
  \notag P\left( t_{new} = 1 \vert \mathbf{X}, \mathbf{t}, \mathbf{x}_{new} = (0, 1, 1) \right) P \left( x_3 = 1  \vert x_1, x_2 \right)
\end{align}

The probability of $x_3$ being either $1$ or $0$ given the other two features is
$\frac{1}{2}$ since we assume that all variables are independent. We then calculate the probability as in $1.1$ and get the answer $\frac{14}{33}$.

% We can calculate this the same way as in $a)$. The only difference is that the
% probability for $\mathbf{x}_{new}$ being content (or not content) now becomes
% the probability of that person being healthy and content plus the probability
% of that person not being healty but content.
%
% In this case this is equivalent to us simply ignoring the ''healthy'' variable
% and just doing the calculation on the remaining two.
%
% The final probablity becomes
%
% \begin{align}
%   \frac{\frac{1}{8}}{\frac{1}{8} + \frac{3}{16}} = \frac{2}{5}
% \end{align}

\subsection{}

Naive Bayes assume that the features are independent but we do not
expect them to be. This is usually not a problem. In this case, however,
$x_1, x_2$ and $x_3$ are not just dependent, they represent the same data.
They are defined by each other.

This means that we cannot for instance classify the potential input-vector
$\mathbf{x} = (1, 1, 0, 0)$ since a person cannot be both younger than $20$ and
between $20$ and $30$.

A solution to this would be to combine those three features into one
categorical feature with three possible values. Now we have a vector with just
two features and we can use naive Bayes with a multinoulli distribution.

\setcounter{section}{2}
\setcounter{subsection}{0}
\subsection{}

\paragraph{a)}

Bayes' theorem gives us the same formula as $(1)$.

\begin{align}
  P\left( t_{new} = 1 \vert \mathbf{X}, \mathbf{t}, \mathbf{x}_{new} \right)
  = \frac{P \left( \mathbf{x}_{new} \vert t_{new} = 1, \mathbf{X}, \mathbf{t} \right) P \left( t_{new} = 1 \right)}{ \sum_j P \left( \mathbf{x}_{new} \vert t_{new} = j, \mathbf{X}, \mathbf{t} \right) P \left( t_{new} = j \right) }
\end{align}

Again we have the same prior for all classes so the $P(t_{new} = x)$ terms cancel out each other. The likelihood function $P(\mathbf{x}_{new}
\vert t_{new} = i, \mathbf{X}, \mathbf{y}) = P(\mathbf{x}_{new} \vert
\hat{\mu}_i, \hat{\sigma}_i^2)$ can be written as $\prod_{d = 1}^{2}
\mathcal{N}(\hat{\mu}_{id}, \hat{\sigma}^2_{id})$.

\paragraph{b, c)}

See code.

\paragraph{d)}

Run problem21.m. Both classifiers passed the test without errors.

\subsection{}

\paragraph{a, b)}

See code.

\paragraph{c)}

Run problem22.m. See table 1 for our results. From reading the
table it seems that calculating the variance is not beneficial. $\sim95\%$
prediction rate vs $\sim75\%$.

\begin{table}
  \begin{tabular}{c | c | c | c | c}
    Test \# & Original data (5) & Orig (8) & Scaled data (5) & Scaled (8)\\
    \hline
    1 & 212 & 197 & 172 & 180\\
    2 & 213 & 208 & 160 & 175\\
    3 & 210 & 208 & 159 & 167\\
    4 & 213 & 200 & 180 & 158\\
    5 & 215 & 202 & 165 & 172\\
  \end{tabular}
\caption{The number of correct predictions for each set of data}
\end{table}

\end{document}
