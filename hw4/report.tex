\documentclass{article}

\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

\pagestyle{fancy}

\author{
  Robin Touche \\
  \and
  Fredrik Bredmar
}

\title{Machine Learning Homework 4}

\begin{document}

\maketitle

\subsection*{1.1}
\paragraph{a)}

Only graph (1).

\paragraph{b)}

The network can have no cycles. Only connections forward from one layer to the
next are allowed.

\subsection*{1.2}

By definition the current network looks like this

\begin{align}
  y = \frac{1}{2}\mathbf{w}_1^T \mathbf{x} + \frac{1}{2}\mathbf{w}_2^T \mathbf{x}
\end{align}

We want to create a network that looks like

\begin{align}
  y = \mathbf{w}_3^T \mathbf{x}
\end{align}

By just setting the outputs as equal (which is exactly what we're after) we get
the equation

\begin{align}
  \nonumber \mathbf{w}_3^T \mathbf{x} &= \frac{1}{2}\mathbf{w}_1^T \mathbf{x} + \frac{1}{2}\mathbf{w}_2^T \mathbf{x} \Leftrightarrow\\
  \mathbf{w}_3^T &= \frac{1}{2}\mathbf{w}_1^T + \frac{1}{2}\mathbf{w}_2^T \Leftrightarrow\\
  \nonumber \mathbf{w}_3 &= \frac{1}{2}\mathbf{w}_1 + \frac{1}{2}\mathbf{w}_2
\end{align}

\subsection*{1.3}

We calculate the gradient of $E = \frac{1}{2} \left(t - y \right)$ with regards
to $\mathbf{w}$, multiply by our learning rate and subtract this from the new
weight. So the new weights becomes

\begin{align}
  w_i = \frac{\partial E}{\partial w_i}
\end{align}

\subsection*{1.4}

\paragraph{a}

\begin{align}
  \frac{\partial E}{\partial z_k} = \frac{dy_k}{dz_k} \frac{\partial E}{\partial y_k} =
  y_k \left( 1 - y_k \right) \frac{\partial E}{\partial y_k}
\end{align}

\paragraph{b}

\begin{align}
  \frac{\partial E}{\partial z_j} = \frac{dy_j}{dz_j} \frac{\partial E}{\partial y_j} =
  y_j \left( 1 - y_j \right) \frac{\partial E}{\partial y_j}
\end{align}

\paragraph{c}

\begin{align}
  \frac{\partial E}{\partial w_{jk}} = \frac{\partial z_k}{\partial w_{jk}} \frac{\partial E}{z_k} =
  y_j \frac{\partial E}{\partial z_k}
\end{align}

\paragraph{d}

\begin{align}
  \frac{\partial E}{\partial w_{ij}} = \frac{\partial z_j}{\partial w_{ij}} \frac{\partial E}{z_j} =
  y_i \frac{\partial E}{\partial z_j}
\end{align}

\subsection*{2.1}

The weight decay part of the cost gradient is the derivative of
\begin{align}
  \frac{d}{d\mathbf{w}}\frac{1}{2}\cdot wd_coefficient \cdot \theta^2 \Rightarrow \\
  \nonumber \frac{\partial}{\partial w_i} = 2 \cdot \frac{1}{2} wd\_coefficient \cdot \theta = wd\_coefficient \theta
\end{align}


\end{document}
